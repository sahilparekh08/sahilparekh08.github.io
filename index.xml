<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Sahil Parekh</title><link>/</link><description>Recent content on Sahil Parekh</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sat, 01 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="/index.xml" rel="self" type="application/rss+xml"/><item><title>Microservices Benchmarking Analysis</title><link>/projects/microservice_benchmarking_analysis/</link><pubDate>Sat, 01 Feb 2025 00:00:00 +0000</pubDate><guid>/projects/microservice_benchmarking_analysis/</guid><description>This repository provides a profiling and benchmarking framework for analyzing the performance of microservices in the DeathStarBench suite. It enables detailed service-level latency breakdowns by combining hardware performance counters, execution profiling, and distributed tracing (via Jaeger).
Key features:
Fine-grained analysis of core-level metrics (LLC loads/misses, instructions retired) Support for containerized deployments via Docker Compose Thread-to-core pinning and cache partitioning (Intel CAT) for isolation and resource control Integration with Jaeger distributed tracing to collect, filter, and analyze traces from specific microservices Automated multi-run test configuration for robust analysis This tool is useful for systems researchers and engineers aiming to:</description><content>&lt;p>This repository provides a profiling and benchmarking framework for analyzing the performance of microservices in the DeathStarBench suite. It enables detailed service-level latency breakdowns by combining hardware performance counters, execution profiling, and distributed tracing (via Jaeger).&lt;/p>
&lt;p>Key features:&lt;/p>
&lt;ul>
&lt;li>Fine-grained analysis of core-level metrics (LLC loads/misses, instructions retired)&lt;/li>
&lt;li>Support for containerized deployments via Docker Compose&lt;/li>
&lt;li>Thread-to-core pinning and cache partitioning (Intel CAT) for isolation and resource control&lt;/li>
&lt;li>Integration with Jaeger distributed tracing to collect, filter, and analyze traces from specific microservices&lt;/li>
&lt;li>Automated multi-run test configuration for robust analysis&lt;/li>
&lt;/ul>
&lt;p>This tool is useful for systems researchers and engineers aiming to:&lt;/p>
&lt;ul>
&lt;li>Evaluate microservice performance bottlenecks under realistic load&lt;/li>
&lt;li>Study the impact of hardware-level resource contention&lt;/li>
&lt;li>Optimize CPU/cache allocation strategies for fine grained resource allocation&lt;/li>
&lt;/ul></content></item><item><title>Crawl And Order, a Distributed Search Engine</title><link>/projects/crawl_and_order/</link><pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate><guid>/projects/crawl_and_order/</guid><description>Crawl and Order is a scalable, cloud-based search engine. The system is designed to efficiently handle user queries and deliver relevant search results. It consists of four major components:
Crawler: Crawls the web and fetches pages. Indexer: Builds an inverted index from the crawled pages. PageRank: Ranks pages based on their importance. Ranker: Queries the inverted index and the pagerank results to compute a final score for every relevant URL and order them in decreasing order of relevance.</description><content>&lt;p>&lt;strong>Crawl and Order&lt;/strong> is a scalable, cloud-based search engine. The system is designed to efficiently handle user queries
and deliver relevant search results. It consists of four major components:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Crawler&lt;/strong>: Crawls the web and fetches pages.&lt;/li>
&lt;li>&lt;strong>Indexer&lt;/strong>: Builds an inverted index from the crawled pages.&lt;/li>
&lt;li>&lt;strong>PageRank&lt;/strong>: Ranks pages based on their importance.&lt;/li>
&lt;li>&lt;strong>Ranker&lt;/strong>: Queries the inverted index and the pagerank results to compute a final score for every relevant URL and order them in decreasing order of relevance.&lt;/li>
&lt;li>&lt;strong>Frontend&lt;/strong>: Provides a user-friendly interface to perform web search and image search.&lt;/li>
&lt;/ol>
&lt;p>Under the hood, the project is built on a scalable distributed Key-Value storage which exposes simple operations like GET, PUT and DELETE. Interaction with the KV store is through a distributed computational framework called Flame, which is similar in its functionality to Apache Spark.&lt;/p>
&lt;p>One can submit a Job (such as crawler.jar) to the Flame Coordinator, which will then distribute the job to multiple Flame Workers. The workers will then fetch the data from the KV store (interacting with the KVS Coordinator and subsequently with relevant KVS Workers), process it according to the functions and lambdas specified by the submitted job and write the results back to the KV store. The KV store is also responsible for storing the intermediate results of the computation.&lt;/p>
&lt;p>The most fundamental unit of computation of the Flame framework are FlameRDDs and FlamePairRDDs. The framework also exposes functions on these RDDs such as map, mapToPair, flatMap, flatMapToPair, foldByKey, filter, join, etc. which can be used to perform complex distributed computations.&lt;/p>
&lt;p>Communication between the components is done using the HTTP protocol. All the components run a multithreaded HTTP webserver to process requests. The KV Store persists data in Protobuf format.&lt;/p>
&lt;p>&lt;strong>Key Features&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Crawler&lt;/th>
&lt;th>Indexer&lt;/th>
&lt;th>PageRank&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Intra-server multithreading&lt;/td>
&lt;td>Indexed URLs on page content, meta tags and site properties&lt;/td>
&lt;td>Optimised memory footprint&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>URL Depth limiting and priority based sampling to ensure host diversity&lt;/td>
&lt;td>Utilised stemming and tokenisation&lt;/td>
&lt;td>Implemented per epoch checkpointing&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Integrated host blacklists&lt;/td>
&lt;td>Built a batch processing mode&lt;/td>
&lt;td>Implemented heuristics to prevent sink URL problem&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;img alt="System Architecture" src="/projects/crawl_and_order/arch.jpg">&lt;/p>
&lt;p>&lt;img alt="High Level Flow" src="/projects/crawl_and_order/high_level_approach.png">&lt;/p>
&lt;p>&lt;img alt="Home Page" src="/projects/crawl_and_order/home_page.png">&lt;/p>
&lt;p>GitHub Repo: &lt;a href="https://github.com/sahilparekh08/Crawl-And-Order">Crawl-And-Order&lt;/a>&lt;/p>
&lt;p>NOTE: This project was a part of the course CIS 5550: Internet and Web Systems (Fall 2024), University of Pennsylvanai&lt;/p></content></item><item><title>University of Pennsylvania</title><link>/experience/upenn/</link><pubDate>Sat, 01 Jun 2024 00:00:00 +0000</pubDate><guid>/experience/upenn/</guid><description>Role I was a Research Assistant with Dr. Linh Thi Xuan Phan where I studied and built scheduling algorithms on Linux based multi-core platforms for safely scheduling real-time tasks during Summer 2024. Her I gained experience on writing and debugging kernel code, building system calls and developing and testing new kerbel scheduling policies such as EDF. I also utilised technologies like Memguard and Intel-CAT for resource allocation and management between tasks running on the kerbel.</description><content>&lt;h3 id="role">Role&lt;/h3>
&lt;p>I was a Research Assistant with Dr. Linh Thi Xuan Phan where I studied and built scheduling algorithms on Linux based multi-core platforms for safely scheduling real-time tasks during Summer 2024. Her I gained experience on writing and debugging kernel code, building system calls and developing and testing new kerbel scheduling policies such as EDF. I also utilised technologies like Memguard and Intel-CAT for resource allocation and management between tasks running on the kerbel.&lt;/p>
&lt;p>I have also served as a Teaching Assistant for the following courses:&lt;/p>
&lt;ol>
&lt;li>CIS 3310 (Fall 2023 and CIS 5510 (Spring 2024) Networks and Security course instructed by Dr. Sebastian Angel&lt;/li>
&lt;li>CIS 5050 (Fall 2024) Software Systems course instructed by Dr. Linh Thi Xuan Phan&lt;/li>
&lt;/ol></content></item><item><title>PennCloud, a Google Suite Replica</title><link>/projects/penncloud/</link><pubDate>Wed, 01 May 2024 00:00:00 +0000</pubDate><guid>/projects/penncloud/</guid><description>PennCloud exemplifies a distributed, scalable, and fault-tolerant system. It encompasses a frontend interface enabling users to send emails to PennCloud users and external users, store and retrieve files and manage related folders and work on collaborative spreadsheets. The system also offers user authentication and password management.
Key terms:
Checkpointing Logging gRPC Dynamic membership of servers load rebalancing Causal Consistency HTTP web server The system accepts requests for any user on a static address which belongs to a load balancer.</description><content>&lt;p>PennCloud exemplifies a distributed, scalable, and fault-tolerant system. It encompasses a frontend interface enabling users to send emails to PennCloud users and external users, store and retrieve files and manage related folders and work on collaborative spreadsheets. The system also offers user authentication and password management.&lt;/p>
&lt;p>Key terms:&lt;/p>
&lt;ul>
&lt;li>Checkpointing&lt;/li>
&lt;li>Logging&lt;/li>
&lt;li>gRPC&lt;/li>
&lt;li>Dynamic membership of servers&lt;/li>
&lt;li>load rebalancing&lt;/li>
&lt;li>Causal Consistency&lt;/li>
&lt;li>HTTP web server&lt;/li>
&lt;/ul>
&lt;p>The system accepts requests for any user on a static address which belongs to a load balancer. The load balancer then redirects the user to an active frontend server which accepts different kinds of HTTP requests. The load balancer keeps track of all active frontend servers, also providing the ability to scale the frontend layer. The user requests are redirected to a backend layer which comprises of stateless backend servers and a Key-Value store, ensuring durability, consistency and availability. Managed by a coordinator, the backend layer supports dynamic membership, allowing for the seamless integration of new backend servers.&lt;/p>
&lt;p>The system also provides an admin console which shows information about the number of active and inactive frontend and backend servers, the partitions assigned to each backend servers and the ability to stop and restart frontend and backend servers to aid in scalability.&lt;/p>
&lt;p>&lt;img alt="System Architecture" src="/projects/penncloud/system_arch.jpg">&lt;/p>
&lt;p>GitHub Repo: &lt;a href="https://github.com/sahilparekh08/PennCloud">PennCloud&lt;/a>&lt;/p>
&lt;p>NOTE: This project was a part of the course CIS 5050: Software Systems (Spring 2024), University of Pennsylvanai&lt;/p></content></item><item><title>Spotify Trending Song Prediction</title><link>/projects/spotify_proj/</link><pubDate>Wed, 01 May 2024 00:00:00 +0000</pubDate><guid>/projects/spotify_proj/</guid><description>The aim of this project is to create a predictive model to determine the viral potential of songs based on a comprehensive analysis of song statistics. To achieve this, we utilized two primary datasets from Spotify. The first dataset contains detailed information on over 1.2 million songs, including their energy index, danceability, liveness, tempo, and other musical features. The second dataset contains information on the performance of songs across various Spotify charts, detailing their presence and duration within these charts throughout different years.</description><content>&lt;p>The aim of this project is to create a predictive model to determine the viral potential of songs based on a comprehensive analysis of song statistics. To achieve this, we utilized two primary datasets from Spotify. The first dataset contains detailed information on over 1.2 million songs, including their energy index, danceability, liveness, tempo, and other musical features. The second dataset contains information on the performance of songs across various Spotify charts, detailing their presence and duration within these charts throughout different years.&lt;/p>
&lt;p>By analyzing these datasets, we developed a robust predictive model that can forecast the likelihood of a song going viral. This was achieved by examining both the intrinsic qualities of the songs and their historical performance on Spotify charts. Each section of our Jupyter Notebook (added to the GitHub repository linked below) is carefully annotated to explain the rationale behind our analysis, the methodologies employed, and the insights derived from the findings.&lt;/p>
&lt;p>The notebook is divided into the following major parts:&lt;/p>
&lt;ol>
&lt;li>Data Loading: This done by loading the aforementioned datasets onto an Apache Spark cluster&lt;/li>
&lt;li>Exploratory Data Analysis: The aim is to analyze various characteristics of songs to forecast their likelihood of becoming popular and making it to the trending charts. We hypothesize that certain song attributes, such as energy, danceability, and tempo, significantly influence their popularity&lt;/li>
&lt;li>Data Cleaning and Pre-processing: This step is a precursor to making the data ready to be fed into our classification models&lt;/li>
&lt;li>Classification Models: We compare three types of classification models which are themselves built iteratively. The 3 models are:
a. Logistic Regression model
b. Neural Network based models
c. Decision Tree based models&lt;/li>
&lt;/ol>
&lt;p>GitHub Repo: &lt;a href="https://github.com/sahilparekh08/Spotify-Trending-Song-Prediction">Spotify-Trending-Song-Prediction&lt;/a>&lt;/p></content></item><item><title>Kafka Streams Application</title><link>/projects/kafka_streams/</link><pubDate>Fri, 01 Mar 2024 00:00:00 +0000</pubDate><guid>/projects/kafka_streams/</guid><description>This is a Java based application demonstrating stream processing applications using Kafka. The app consists of a bash script giving the user flexibility to spin up their own Apache Kafka cluster with configurable number of machines, replicatino factor and the ability to create their own topics. It also provides the users a Producer and Consumer API, the ability to read PDF files word by word using Apache PDFBox. Gradle is used for dependency management.</description><content>&lt;p>This is a Java based application demonstrating stream processing applications using Kafka. The app consists of a bash script giving the user flexibility to spin up their own Apache Kafka cluster with configurable number of machines, replicatino factor and the ability to create their own topics. It also provides the users a Producer and Consumer API, the ability to read PDF files word by word using Apache PDFBox. Gradle is used for dependency management. The application shows a word count example utilizing a sliding window property of streams and fetching the top N highest occuring non-ordinary and ordinary (articles, pronouns, prepositions, etc) words in a collection of PDF files.&lt;/p>
&lt;p>GitHub Repo: &lt;a href="https://github.com/sahilparekh08/Kafka-Streams-App">Kafka-Streams-App&lt;/a>&lt;/p>
&lt;p>Refernces:&lt;/p>
&lt;ol>
&lt;li>&lt;a href="chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://www.microsoft.com/en-us/research/wp-content/uploads/2017/09/Kafka.pdf">Kafka: a Distributed Messaging System for Log Processing&lt;/a> by Jay Kreps, Neha Narkhede, Jun Rao; NetDB workshop &amp;lsquo;11, 2011&lt;/li>
&lt;li>&lt;a href="https://dl.acm.org/doi/10.1145/3242153.3242155">Streams and Tables: Two Sides of the Same Coin&lt;/a> by Matthias J. Sax, Guozhang Wang, Matthias Weidlich, Johann-Christoph Freytag; BIRTE &amp;lsquo;18: Proceedings of the International Workshop on Real-Time Business Intelligence and Analytics, 2018&lt;/li>
&lt;li>&lt;a href="https://kafka.apache.org/37/documentation/streams/core-concepts">https://kafka.apache.org/37/documentation/streams/core-concepts&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kafka.apache.org/20/documentation/streams/developer-guide/dsl-api.html">https://kafka.apache.org/20/documentation/streams/developer-guide/dsl-api.html&lt;/a>&lt;/li>
&lt;/ol></content></item><item><title>Algorithmic Strategies for Competitive Games</title><link>/projects/algo_games/</link><pubDate>Fri, 01 Dec 2023 00:00:00 +0000</pubDate><guid>/projects/algo_games/</guid><description>The aim of the projects was to iteratively build on algorithmic strategies and performance optimisations to submit a &amp;ldquo;player&amp;rdquo; for each game by periodically testing your strategies against other teams.
The 4 projects involved as a part of this are:
Parallel Football Organisms Seven Mosquito These projects are a part of the course CIS 5590: Programming and Problem Solving (Fall 2023), University of Pennsylvania. Code for these projects cannot be uploaded to GitHub to adhere to course policies that prevent publishing code to adhere with the academic integrity requirements.</description><content>&lt;p>The aim of the projects was to iteratively build on algorithmic strategies and performance optimisations to submit a &amp;ldquo;player&amp;rdquo; for each game by periodically testing your strategies against other teams.&lt;/p>
&lt;p>The 4 projects involved as a part of this are:&lt;/p>
&lt;ol>
&lt;li>Parallel Football&lt;/li>
&lt;li>Organisms&lt;/li>
&lt;li>Seven&lt;/li>
&lt;li>Mosquito&lt;/li>
&lt;/ol>
&lt;p>These projects are a part of the course CIS 5590: Programming and Problem Solving (Fall 2023), University of Pennsylvania. Code for these projects cannot be uploaded to GitHub to adhere to course policies that prevent publishing code to adhere with the academic integrity requirements. Please reach out to me for questions regarding implementation details.&lt;/p>
&lt;h2 id="1-parallel-football">1. Parallel Football&lt;/h2>
&lt;p>Parallel Football is a team game, played on a 32x32 grid by four teams. Each team has P players, each occupying a cell, and a home cell located at a corner of the grid. Each cell except for the home cells begins with a football, totaling 1020 footballs overall. The objective is to kick balls to the team&amp;rsquo;s home cell. Teams designate their players&amp;rsquo; initial positions. Players can move one cell in any direction or kick a football within a K-cell range. Although cells may contain multiple balls, players can kick only one ball at a time. However, multiple players can kick simultaneously if enough balls are present. Scoring is achieved when a ball reaches a home cell, and the aim is to score as many goals as possible. To accomplish this, teams must strategically define their players&amp;rsquo; initial positions and movements.&lt;/p>
&lt;p>&lt;img alt="Game Board Screenshot" src="/projects/algo_games/parallel_football.png">&lt;/p>
&lt;h2 id="2-organisms">2. Organisms&lt;/h2>
&lt;p>‚ÄúOrganisms‚Äù is a virtual ecosystem, manifested as a grid. Organisms on the grid rely on energy to survive, which they can obtain from eating food scattered across the grid. As these organisms move, reproduce, and interact with their environment, they adhere to specific rules and constraints. It is a game with imperfect information where each organism can only ‚Äúsee‚Äù in the four orthogonal directions, which gives them information about the presence of food and other organisms on a neighboring square. Each organism also has a number between 0 and 255 called its external state which is viewable by the organisms on the neighboring squares. The essence of this project is to observe, understand, and predict how these organisms will behave, adapt, and evolve over time in a world filled with opportunities and threat&lt;/p>
&lt;p>There are multiple ways to define success in the context of this project. Our group‚Äôs goal was to create an adaptable, versatile brain that is able to survive in many different environments.&lt;/p>
&lt;p>&lt;img alt="Game Board Screenshot" src="/projects/algo_games/organisms.png">&lt;/p>
&lt;h2 id="3-seven">3. Seven&lt;/h2>
&lt;p>Seven, a Scrabble inspired game entails creating a player whose task is to bid on letters in an efficient manner to make high-scoring words. Players bid on letters from a common pool of 98 letters and aim to create high-scoring words, preferably creating words with a score higher than what the player spent to create that word. Starting from a score of 100, each player plays for n rounds with 8p bids per round where p is the number of players in the game. Each letter has a score attached to it which is directly proportional to how rare the letter is in the English language. Players can make up to 7 letter words with 7 letter words bagging them additional 50 points. Each game can also be played with 0 to 6 secret letters that are dealt to each player randomly at the start of every round. These secret letters are taken from the initial common pool of 96 letters itself. Each bid is submitted simultaneously and the winning bid has to pay the amount of the second-highest bid for the letter up for auction, following the rules of Vickrey auction. In a round, letters are auctioned without replacement. In the event of a tie, the simulator randomly selects a winner amongst the top bidders. Words are deemed valid by checking them against a list of 1 to 7 letter valid word lists and are awarded respective scores&lt;/p>
&lt;p>The team created a player whose bidding strategy relied on a combination of trying to make 7 letter words by using the letters efficiently by determining the best ‚Äòn‚Äô future words we can make at every bidding round and a ‚Äòmemory‚Äô heuristic which kept track of the previous winning and highest bids made for the letter over the rounds&lt;/p>
&lt;p>&lt;img alt="Game Board Screenshot" src="/projects/algo_games/seven.png">&lt;/p>
&lt;h2 id="4-mosquito">4. Mosquito&lt;/h2>
&lt;p>Mosquito is a single player game with very simple gameplay. Before a game starts, the player gets access to the board‚Äôs layout, which includes a set of walls that are guaranteed not to completely partition the board. The player is also told how many lights they can place. The goal is to place these lights anywhere on the board so that they capture 50% randomly placed mosquitos as quickly as possible. The player can also place one collector, and when a mosquito flies over the collector, they are captured and removed from the board. The player must submit their placements before the game starts, and cannot edit them afterwards.&lt;/p>
&lt;p>The team developed a player called Itchy Infantry that has a greedy approach to Mosquito. Itchy Infantry simulates many rounds and returns the best simulated round‚Äôs lights and collector. Within the simulation, it places the first light and the collector in what it perceives as one of the most open spots, and has a greedy approach afterwards, placing lights in the position that covers the most open area.&lt;/p>
&lt;p>&lt;img alt="Game Board Screenshot" src="/projects/algo_games/mosquito.png">&lt;/p></content></item><item><title>JP Morgan Chase &amp; Co.</title><link>/experience/jpmc/</link><pubDate>Fri, 07 Jul 2023 00:00:00 +0000</pubDate><guid>/experience/jpmc/</guid><description>Role I started my journey with JP Morgan Chase &amp;amp; Co. in their investment banking regulatory reporting department as an intern during my last undergraduate semester. I started by understanding the domain of regulatory reporting and the trade life cycle. Since our team handled real-time regulatory reporting for the investment firm, I got to work on real-time low-latency systems, streaming and batch frameworks and cloud microservice architectures with multiple scalable components.</description><content>&lt;h3 id="role">Role&lt;/h3>
&lt;p>I started my journey with JP Morgan Chase &amp;amp; Co. in their investment banking regulatory reporting department as an intern during my last undergraduate semester. I started by understanding the domain of regulatory reporting and the trade life cycle. Since our team handled real-time regulatory reporting for the investment firm, I got to work on real-time low-latency systems, streaming and batch frameworks and cloud microservice architectures with multiple scalable components. I also worked on its own custom streaming and batch framework to gain a processing throughput of millions of messages a day with sub-second end-to-end latencies per message. I also used docker and Kubernetes to deploy our microservices on a private cloud infrastructure and on-premise data centers. With developing these models from scratch, I learnt a lot about system design, analysing frameworks, streaming and batch systems and how to find, analyse and remediate performance bottlenecks.&lt;/p>
&lt;h3 id="positions">Positions&lt;/h3>
&lt;ol>
&lt;li>Software Engineer II (Feb 2023 - Jul 2023)&lt;/li>
&lt;li>Software Engineer I (Aug 2020 - Jan 2023)&lt;/li>
&lt;li>Software Engineer Intern (Jan 2020 - Jul 2020)&lt;/li>
&lt;/ol>
&lt;p>Some of the technologies I used extensively:&lt;/p>
&lt;ol>
&lt;li>Languages: Java, SQL, bash, Python&lt;/li>
&lt;li>Frameworks and Technologies: Spring, JUnit, Mockito, Apache Kafka, FIX Engine, MongoDB, EhCache, Oracle BDB, Hadoop, Oracle Exadata&lt;/li>
&lt;li>Tools: Docker, Kubernetes, AWS, Jenkins, Control-M, Splunk, Dynatrace, Gradle, Maven, Git&lt;/li>
&lt;/ol></content></item><item><title>Learning Management System</title><link>/projects/lms/</link><pubDate>Sun, 01 Aug 2021 00:00:00 +0000</pubDate><guid>/projects/lms/</guid><description>Circa 2020.
It was the middle of the Covid-19 global pandemic and among the worst hit in the education sector were underprivileged children in school. The goal was to make a cost-effective and easily manageable learning management system which could enable instructors and schools to impart education and students to learn from their houses. A small team comprising of junior (including me) and senior developers from JP Morgan Chase collaborated with Tata Institute of Social Sciences to create this system.</description><content>&lt;p>&lt;em>Circa 2020&lt;/em>.&lt;/p>
&lt;p>It was the middle of the Covid-19 global pandemic and among the worst hit in the education sector were underprivileged children in school. The goal was to make a cost-effective and easily manageable learning management system which could enable instructors and schools to impart education and students to learn from their houses. A small team comprising of junior (including me) and senior developers from JP Morgan Chase collaborated with Tata Institute of Social Sciences to create this system. It was deployed on an AWS infrastructure using DynamoDB and S3 as its storage components with the application written in ReactJS and NodeJS.&lt;/p>
&lt;p>The application consisted of a student and instructor portal which were analogous to a customer and admin portal. Each school could have its own instructor portal, course management systems and grading systems. Students could register as a part of a school or as individuals to learn publicly available courses. The application was also made to be lightweight and consume low bandwidth especially during video streaming so as to perform well in poor network conditions.&lt;/p>
&lt;p>I worked on the backend development, primarily on the API structure design, development, testing and connectivity to the front end. I also worked on designing the AWS infrastructure, configuring security policies and setting up S3 and DynamoDB after doing a cost analysis.&lt;/p>
&lt;p>&lt;img alt="Home Screen" src="/projects/lms/lms_home.png">&lt;/p>
&lt;p>&lt;img alt="Course Screen" src="/projects/lms/lms_course.png">&lt;/p>
&lt;p>&lt;img alt="Learning Path Screen" src="/projects/lms/lms_learning_path.png">&lt;/p>
&lt;p>&lt;img alt="Admin Console Screen" src="/projects/lms/lms_admin.png">&lt;/p></content></item><item><title>An Approach to Reducing Uncertainty Problem in NIDS</title><link>/projects/nids/</link><pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate><guid>/projects/nids/</guid><description>Detecting malicious packets on the fly with minimal lag time is a very important feature of a Network Intrusion Detection System. With the use of machine learning algorithms, the detection becomes more robust as it aids the rules coded into the system and also helps in detecting packets which might be malicious for which rules have not been written yet.
The project&amp;rsquo;s goal was to train the system on a variety of malicious and non-malicious packets and then build a model through which it can then predict and bucket the packet.</description><content>&lt;p>Detecting malicious packets on the fly with minimal lag time is a very important feature of a Network Intrusion Detection System. With the use of machine learning algorithms, the detection becomes more robust as it aids the rules coded into the system and also helps in detecting packets which might be malicious for which rules have not been written yet.&lt;/p>
&lt;p>The project&amp;rsquo;s goal was to train the system on a variety of malicious and non-malicious packets and then build a model through which it can then predict and bucket the packet. The first layer of this project uses a Genetic Algorithm as a feature selection algorithm on a combination of KDDCup99 Dataset, NSL KDD data set and packets collected through Wireshark by simulating cyber attacks in a LAN. By using a fitness function on a random forest classifier which rewards a chromosome for high accuracy and low false positives and error rates, the data is run through multiple epochs of fitness, selection, crossover and mutation functions to determine the important features from a 43 feature dataset.&lt;/p>
&lt;p>The resulting data was then passed through multiple classification algorithms for a comparitive study. This project was published as an &lt;a href="https://ieeexplore.ieee.org/document/9342634">IEEE paper&lt;/a> in the 15th IEEE Conference on Industrial and Information Systems.&lt;/p></content></item><item><title>Connect</title><link>/connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/connect/</guid><description>Reach out to me to discuss anything Data, Systems, Tech (even Music) or just drop in to say hi! üíº LinkedIn üñ•Ô∏è GitHub üìß sahilparekh08@gmail.com üìç Philadelphia, USA || Mumbai, India</description><content>&lt;h4 id="reach-out-to-me-to-discuss-anything-data-systems-tech-even-music-or-just-drop-in-to-say-hi">Reach out to me to discuss anything Data, Systems, Tech (even Music) or just drop in to say hi!&lt;/h4>
&lt;ul>
&lt;li>üíº &lt;a href="https://www.linkedin.com/in/parekh-sahil/">LinkedIn&lt;/a>&lt;/li>
&lt;li>üñ•Ô∏è &lt;a href="https://github.com/sahilparekh08">GitHub&lt;/a>&lt;/li>
&lt;li>üìß &lt;a href="mailto:sahilparekh08@gmail.com">sahilparekh08@gmail.com&lt;/a>&lt;/li>
&lt;li>üìç Philadelphia, USA || Mumbai, India&lt;/li>
&lt;/ul></content></item><item><title>Resume</title><link>/resume/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/resume/</guid><description/><content>&lt;script type="text/javascript" src= '/js/pdf-js/build/pdf.js'>&lt;/script>
&lt;link href="/fontawesome/css/all.min.css" rel="stylesheet">
&lt;style>
#embed-pdf-container {
position: relative;
width: 100%;
height: auto;
}
.pdf-canvas {
border: 1px solid black;
direction: ltr;
width: 100%;
height: auto;
display: none;
}
#loadingWrapper {
display: none;
justify-content: center;
align-items: center;
width: 100%;
height: 350px;
}
#loading {
display: inline-block;
width: 50px;
height: 50px;
border: 3px solid #d2d0d0;;
border-radius: 50%;
border-top-color: #383838;
animation: spin 1s ease-in-out infinite;
-webkit-animation: spin 1s ease-in-out infinite;
}
#overlayText > a:link{
color: steelblue;
}
#overlayText {
position: absolute;
word-wrap: break-word;
right: 5px;
top: 5px;
}
@keyframes spin {
to { -webkit-transform: rotate(360deg); }
}
@-webkit-keyframes spin {
to { -webkit-transform: rotate(360deg); }
}
&lt;/style>
&lt;div id="embed-pdf-container">
&lt;div id="loadingWrapper">
&lt;div id="loading">&lt;/div>
&lt;/div>
&lt;div id="overlayText">
&lt;a href="/resume.pdf" download>
&lt;i class="fas fa-download fa-2x">&lt;/i>
&lt;/a>
&lt;/div>
&lt;/div>
&lt;script type="text/javascript">
window.onload = function() {
var url = 'https:\/\/sahilparekh08.github.io\/resume.pdf';
console.log(url)
var pdfjsLib = window['pdfjs-dist/build/pdf'];
pdfjsLib.GlobalWorkerOptions.workerSrc = "\/" + '/js/pdf-js/build/pdf.worker.js';
var pdfDoc = null,
pageNum = 1,
pageRendering = false,
pageNumPending = null,
scale = 3,
loadingWrapper = document.getElementById('loadingWrapper');
showLoader();
function renderPage(num) {
pageRendering = true;
pdfDoc.getPage(num).then(function(page) {
var pdf_cont = document.getElementById('embed-pdf-container');
var canvas = getCanvas(num);
pdf_cont.appendChild(canvas);
var viewport = page.getViewport({scale: scale});
canvas.height = viewport.height;
canvas.width = viewport.width;
var ctx = canvas.getContext('2d');
var renderContext = {
canvasContext: ctx,
viewport: viewport
};
var renderTask = page.render(renderContext);
renderTask.promise.then(function() {
pageRendering = false;
showContent(canvas);
if (pageNumPending !== null) {
renderPage(pageNumPending);
pageNumPending = null;
}
});
});
}
function renderAllPages() {
for (var i = 1; i &lt;= pdfDoc.numPages; i++) {
renderPage(i);
}
}
function showContent(canvas) {
loadingWrapper.style.display = 'none';
canvas.style.display = 'block';
}
function showLoader() {
loadingWrapper.style.display = 'flex';
}
function queueRenderPage(num) {
if (pageRendering) {
pageNumPending = num;
} else {
renderPage(num);
}
}
function getCanvas(num) {
var canvasId = 'embed-pdf-container-' + num;
var canvas = document.createElement("canvas");
canvas.id = canvasId;
canvas.className = 'pdf-canvas';
return canvas
}
pdfjsLib.getDocument(url).promise.then(function(pdfDoc_) {
pdfDoc = pdfDoc_;
renderAllPages();
});
}
&lt;/script></content></item></channel></rss>